{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d7e1d7-accb-4207-a342-449223744ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "sites = ['www.newslaundry.com', 'www.theprint.in', 'www.opindia.com', 'www.republicworld.com', 'www.indianexpress.com']\n",
    "for site in sites:\n",
    "    modified_site = site.replace('www.', '').replace('.in', '').replace('.com', '')\n",
    "    file_path = modified_site + '.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    text = []\n",
    "    for index, row in df.iterrows():\n",
    "        text.append(row['text'])\n",
    "\n",
    "    text = [t for t in text if isinstance(t, str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b2aa6-333b-424c-9973-27018c359726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551dc9c-ad8f-47fd-941e-e62bd51473aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def rem_en(input_txt):\n",
    "    words = input_txt.lower().split()\n",
    "    noise_free_words = [word for word in words if word not in stop] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "\n",
    "text = list(map(lambda x: rem_en(x), text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbadbc1-ca38-4253-9b6d-dfcc2164bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "text1 = text\n",
    "tokeniser = RegexpTokenizer(r'\\w+')\n",
    "# df[\"clean_text\"] = df[\"clean_text\"].apply(lambda x: tokeniser.tokenize(x))\n",
    "text1 = list(map(lambda x: tokeniser.tokenize(x), text1))\n",
    "# print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d20c2d-d185-41fa-a4be-57565c261ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "# df[\"clean_text\"] = df[\"clean_text\"].apply(lambda tokens: [lemmatiser.lemmatize(token, pos='v') for token in tokens])\n",
    "text1 = list(map(lambda tokens: [lemmatiser.lemmatize(token, pos='v') for token in tokens], text1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
